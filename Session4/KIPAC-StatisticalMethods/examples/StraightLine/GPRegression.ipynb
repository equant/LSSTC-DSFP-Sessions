{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian Process Regression\n",
    "\n",
    "In this example, we return to our \"straight line\" mock dataset, and investigate a \"model-free model\" for it: a Gaussian Process. The idea is to find a flexible model that can _interpolate_ between the data we have, in order to predict future data lying in the gaps, or beyond the observed domain.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You will need to `pip install scikit-learn` and check that you have v0.18 or higher as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "plt.rcParams['savefig.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Data\n",
    "\n",
    "* As before, let's generate a simple Cepheids-like dataset: observations of $y$ with reported uncertainties $\\sigma_y$, at given $x$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from straightline_utils import *\n",
    "\n",
    "(x, y, sigmay) = generate_data(seed=13)\n",
    "\n",
    "plot_yerr(x, y, sigmay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a Gaussian Process\n",
    "\n",
    "Let's follow [Jake VanderPlas' example](http://www.astroml.org/book_figures/chapter8/fig_gp_example.html#book-fig-chapter8-fig-gp-example), to see how to work with the [`scikit-learn` v0.18](http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html#gaussian-processes-regression-basic-introductory-example) Gaussian Process regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF as SquaredExponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining a GP\n",
    "\n",
    "First we define a kernel function, for populating the covariance matrix of our GP. To avoid confusion, a Gaussian kernel is referred to as a \"squared exponential\" (or a \"radial basis function\", RBF). The squared exponential kernel has one hyper-parameter, the length scale that is the Gaussian width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = 10.0\n",
    "\n",
    "kernel = SquaredExponential(length_scale=h, length_scale_bounds=(0.01, 1000.0))\n",
    "gp0 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's draw some samples from the unconstrained process. Each sample is a function $y(x)$, which we evaluate on a grid. We'll need to assert a value for the kernel hyperparameter $h$, which dictates the correlation length between the datapoints. That will allow us to compute a mean function (which for simplicity we'll set to the mean observed $y$ value), and a covariance matrix that captures the correlations between datapoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xgrid = np.atleast_2d(np.linspace(0, 399, 100)).T\n",
    "print(\"y(x) will be predicted on a grid of length\", len(xgrid))\n",
    "\n",
    "# Draw three sample y(x) functions:\n",
    "draws = gp0.sample_y(xgrid, n_samples=3)\n",
    "\n",
    "print(\"Drew 3 samples, stored in an array with shape \", draws.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot these, to see what our prior looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start a 4-panel figure:\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot our three prior draws:\n",
    "ax = fig.add_subplot(221)\n",
    "ax.plot(xgrid, draws[:,0], '-r')\n",
    "ax.plot(xgrid, draws[:,1], '-g')\n",
    "ax.plot(xgrid, draws[:,2], '-b', label='Rescaled prior sample $y(x)$')\n",
    "ax.set_xlim(0, 399)\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y(x)$')\n",
    "ax.legend(fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each predicted $y(x)$ is drawn from a Gaussian of unit variance, and with off-diagonal elements determined by the covariance function. Try changing `h` to see what happens to the smoothness of the predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For our data to be well interpolated by this Gaussian Process, it will need to be rescaled such that it has zero mean and unit variance. There are [standard methods for doing this](http://scikit-learn.org/stable/modules/preprocessing.html), but we'll do this rescaling here for transparency - and so we know what to add back in later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Rescaler():\n",
    "    def __init__(self, y, err):\n",
    "        self.original_data = y\n",
    "        self.original_err = err\n",
    "        self.mean = np.mean(y)\n",
    "        self.std = np.std(y)\n",
    "        self.transform()\n",
    "        return\n",
    "    def transform(self):\n",
    "        self.y = (self.original_data - self.mean) / self.std\n",
    "        self.err = self.original_err / self.std\n",
    "        return()\n",
    "    def invert(self, scaled_y, scaled_err):\n",
    "        return (scaled_y * self.std + self.mean, scaled_err * self.std)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rescaled = Rescaler(y, sigmay)\n",
    "print('Mean, variance of rescaled data: ',np.round(np.mean(rescaled.y)), np.round(np.var(rescaled.y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we can undo the scaling, for any `y` and `sigmay`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y2, sigmay2 = rescaled.invert(rescaled.y, rescaled.err)\n",
    "print('Maximum differences in y, sigmay, after round trip: ',np.max(np.abs(y2 - y)), np.max(np.abs(sigmay2 - sigmay)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constraining the GP\n",
    "\n",
    "Now, using the same covariance function, lets \"fit\" the GP by constraining each draw from the GP to go through our data points. Let's first look at how this would work for two data points with no uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Choose two of our datapoints:\n",
    "x1 = np.array([x[10], x[12]])\n",
    "rescaled_y1 = np.array([rescaled.y[10], rescaled.y[12]])\n",
    "rescaled_sigmay1 = np.array([rescaled.err[10], rescaled.err[12]])\n",
    "\n",
    "# Instantiate a GP model:\n",
    "kernel = SquaredExponential(length_scale=10.0, length_scale_bounds=(0.01, 1000.0))\n",
    "gp1 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "\n",
    "# Fit it to our two noiseless datapoints:\n",
    "gp1.fit(x1[:, None], rescaled_y1)\n",
    "\n",
    "# We have fit for the length scale parameter: print the result here:\n",
    "params = gp1.kernel_.get_params()\n",
    "print('Best-fit kernel length scale =', params['length_scale'],'cf. input',10.0)\n",
    "\n",
    "# Now predict y(x) everywhere on our xgrid: \n",
    "rescaled_ygrid1, rescaled_ygrid1_err = gp1.predict(xgrid, return_std=True)\n",
    "\n",
    "# And undo scaling:\n",
    "ygrid1, ygrid1_err = rescaled.invert(rescaled_ygrid1, rescaled_ygrid1_err)\n",
    "y1, sigmay1 = rescaled.invert(rescaled_y1, rescaled_sigmay1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = fig.add_subplot(222)\n",
    "ax.plot(xgrid, ygrid1, '-', color='gray', label='Posterior mean $y(x)$')\n",
    "ax.fill(np.concatenate([xgrid, xgrid[::-1]]),\n",
    "        np.concatenate([(ygrid1 - ygrid1_err), (ygrid1 + ygrid1_err)[::-1]]),\n",
    "        alpha=0.3, fc='gray', ec='None', label='68% confidence interval')\n",
    "ax.plot(x1, y1, '.k', ms=6, label='Noiseless constraints')\n",
    "ax.set_xlim(0, 399)\n",
    "ax.set_ylim(0, 399)\n",
    "ax.set_xlabel('$x$')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the absence of information, the GP tends to produce $y(x)$ that fluctuate around the prior mean function, which we chose to be a constant. Let's draw some samples from the posterior PDF, and overlay them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "draws = gp1.sample_y(xgrid, n_samples=3)\n",
    "for k in range(3):\n",
    "    draws[:,k], dummy = rescaled.invert(draws[:,k], np.zeros(len(xgrid)))\n",
    "\n",
    "ax.plot(xgrid, draws[:,0], '-r')\n",
    "ax.plot(xgrid, draws[:,1], '-g')\n",
    "ax.plot(xgrid, draws[:,2], '-b', label='Posterior sample $y(x)$')\n",
    "ax.legend(fontsize=8)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "See how the posterior sample $y(x)$ functions all pass through the constrained points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Including Observational Uncertainties\n",
    "\n",
    "The mechanism for including uncertainties is a little esoteric: `scikit-learn` wants to be given a \"nugget,\" `alpha`, to multiply the diagonal elements of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Choose two of our datapoints:\n",
    "x2 = np.array([x[10], x[12]])\n",
    "rescaled_y2 = np.array([rescaled.y[10], rescaled.y[12]])\n",
    "rescaled_sigmay2 = np.array([rescaled.err[10], rescaled.err[12]])\n",
    "\n",
    "# Instantiate a GP model, including observational errors:\n",
    "kernel = SquaredExponential(length_scale=10.0, length_scale_bounds=(0.01, 1000.0))\n",
    "gp2 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9, \n",
    "                               alpha=(rescaled_sigmay2 / rescaled_y2) ** 2,\n",
    "                               random_state=0)\n",
    "\n",
    "# Fit it to our two noisy datapoints:\n",
    "gp2.fit(x2[:, None], rescaled_y2)\n",
    "\n",
    "# We have fit for the length scale parameter: print the result here:\n",
    "params = gp2.kernel_.get_params()\n",
    "print('Best-fit kernel length scale =', params['length_scale'],'cf. input',10.0)\n",
    "\n",
    "# Now predict y(x) everywhere on our xgrid: \n",
    "rescaled_ygrid2, rescaled_ygrid2_err = gp2.predict(xgrid, return_std=True)\n",
    "\n",
    "# And undo scaling:\n",
    "ygrid2, ygrid2_err = rescaled.invert(rescaled_ygrid2, rescaled_ygrid2_err)\n",
    "y2, sigmay2 = rescaled.invert(rescaled_y2, rescaled_sigmay2)\n",
    "\n",
    "# Draw three posterior sample y(x):\n",
    "draws = gp2.sample_y(xgrid, n_samples=3)\n",
    "for k in range(3):\n",
    "    draws[:,k], dummy = rescaled.invert(draws[:,k], np.zeros(len(xgrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = fig.add_subplot(223)\n",
    "\n",
    "def gp_plot(ax, xx, yy, ee, datax, datay, datae, samples, legend=True):\n",
    "    ax.plot(xx, yy, '-', color='gray', label='Posterior mean $y(x)$')\n",
    "    ax.fill(np.concatenate([xx, xx[::-1]]),\n",
    "            np.concatenate([(yy - ee), (yy + ee)[::-1]]),\n",
    "            alpha=0.3, fc='gray', ec='None', label='68% confidence interval')\n",
    "    ax.errorbar(datax, datay, datae, fmt='.k', ms=6, label='Noisy constraints')\n",
    "    ax.set_xlim(0, 399)\n",
    "    ax.set_ylim(0, 399)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y(x)$')\n",
    "    ax.plot(xgrid, samples[:,0], '-r')\n",
    "    ax.plot(xgrid, samples[:,1], '-g')\n",
    "    ax.plot(xgrid, samples[:,2], '-b', label='Posterior sample $y(x)$')\n",
    "    if legend: ax.legend(fontsize=8)\n",
    "    return\n",
    "\n",
    "gp_plot(ax, xgrid, ygrid2, ygrid2_err, x2, y2, sigmay2, draws, legend=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, the posterior sample $y(x)$ functions pass through the constraints _within the errors_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using all the Data\n",
    "\n",
    "Now let's extend the above example to use all of our datapoints. This additional information should pull the predictions further away from the initial mean function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Use all of our datapoints:\n",
    "x3 = x\n",
    "rescaled_y3 = rescaled.y\n",
    "rescaled_sigmay3 = rescaled.err\n",
    "\n",
    "# Instantiate a GP model, including observational errors:\n",
    "kernel = SquaredExponential(length_scale=10.0, length_scale_bounds=(0.01, 1000.0))\n",
    "gp3 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9, \n",
    "                               alpha=(rescaled_sigmay3 / rescaled_y3) ** 2,\n",
    "                               random_state=0)\n",
    "\n",
    "# Fit it to our noisy datapoints:\n",
    "gp3.fit(x3[:, None], rescaled_y3)\n",
    "\n",
    "# Now predict y(x) everywhere on our xgrid: \n",
    "rescaled_ygrid3, rescaled_ygrid3_err = gp3.predict(xgrid, return_std=True)\n",
    "\n",
    "# And undo scaling:\n",
    "ygrid3, ygrid3_err = rescaled.invert(rescaled_ygrid3, rescaled_ygrid3_err)\n",
    "y3, sigmay3 = rescaled.invert(rescaled_y3, rescaled_sigmay3)\n",
    "\n",
    "# We have fitted the length scale parameter - print the result here:\n",
    "params = gp3.kernel_.get_params()\n",
    "print('Best-fit kernel length scale =', params['length_scale'],'cf. input',10.0)\n",
    "\n",
    "# Draw three posterior sample y(x):\n",
    "draws = gp3.sample_y(xgrid, n_samples=3)\n",
    "for k in range(3):\n",
    "    draws[:,k], dummy = rescaled.invert(draws[:,k], np.zeros(len(xgrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = fig.add_subplot(224)\n",
    "\n",
    "gp_plot(ax, xgrid, ygrid3, ygrid3_err, x3, y3, sigmay3, draws, legend=False)\n",
    "fig\n",
    "\n",
    "# fig.savefig('../../graphics/mfm_gp_example_pjm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We now see the Gaussian Process model providing a smooth interpolation between the points. The posterior samples show fluctuations, but all are plausible under our assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Endnotes\n",
    "\n",
    "* Gaussian processes provide a very flexible way of modeling noisy data such that predictions can be made (interpolation and extrapolation) without making strong assumptions about the underlying generative model\n",
    "\n",
    "\n",
    "* GP predictions involve operations with $N \\times N$ matrices, where $N$ is the number of constraints, while arginalizing over GP hyper-parameters and comparing GP kernels involves computing the determinants of these large matrices: scaling to large datasets may be difficult."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
