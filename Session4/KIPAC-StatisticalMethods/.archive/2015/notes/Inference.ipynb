{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference: Learning from Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We saw that:\n",
    "\n",
    "* Forward models allow us to generate mock datasets, and begin exploring the ability of those models to \"fit\" the data.\n",
    "\n",
    "\n",
    "* Generating mock data involves drawing *datasets* from the *sampling distribution*, a process that can be illustrated with a *probabilistic graphical model.*\n",
    "\n",
    "\n",
    "* Inference is the inverse problem: how to learn plausible values of the model parameters given one, constant, dataset.\n",
    "\n",
    "\n",
    "* *Probability theory* provides a logical framework for dealing with the *assumptions that we necessarily have to make* when working with scientific models, and also *our uncertainty, or degree of belief, about the values of those models' parameters.*\n",
    "\n",
    "\n",
    "* Learning from data involves assigning and evaluating a *prior PDF,* and then updating it into a *posterior PDF* by computing the *value of the likelihood function,*  in a process summarized by Bayes Theorem.\n",
    "\n",
    "\n",
    "* The unnormalized posterior PDF is a function that can be calculated, given assumed functional forms for the likelihood and prior, anywhere in parameter space. \n",
    "\n",
    "\n",
    "* While a posterior PDF is the general solution to an inverse problem, we often need to compress it into a handful of numbers, which need to be chosen with care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayesian statistics, there is no *ad hoc* recipe book beyond the rules of probability theory. \n",
    "\n",
    "\n",
    "* One of its major benefits is that it requires you to *derive* the inference given the assumptions that you make.\n",
    "\n",
    "\n",
    "* PGMs can be a useful guide in such derivations, as you work through the *conditional dependences* of the data and parameters.\n",
    "\n",
    "\n",
    "* The inference you derive will be the only answer to a well-posed problem: if we all make the same assumptions, we will all obtain the same posterior PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
