{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Avoiding Fooling Ourselves: Detection, Fishing, and Blinding\n",
    "\n",
    "Goals:\n",
    "\n",
    "* See how discoveries can be assessed in both the Bayesian and Frequentist frameworks\n",
    "\n",
    "* Understand the dangers of fishing expeditions\n",
    "\n",
    "* Understand unconscious experimenter bias, and how blinding can mitigate it\n",
    "\n",
    "\n",
    "_The first principle [of science] is that you must not fool yourself — and you are the easiest person to fool.”_ - Richard Feynman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further Reading\n",
    "\n",
    "* James Berger, [\"The Bayesian Approach to Discovery\"](https://indico.cern.ch/event/107747/contributions/32678/attachments/24371/35060/berger.pdf) (PHYSTAT 2011)\n",
    "\n",
    "* Kyle Cranmer, [\"Practical Statistics for the LHC\"](https://arxiv.org/pdf/1503.07622.pdf)\n",
    "\n",
    "* Gross & Vitells (2010), [\"Trial factors for the look elsewhere effect in high energy physics\"](https://arxiv.org/pdf/1005.1891.pdf)\n",
    "\n",
    "* MacCoun & Perlmutter (2015), [\"Hide Results to Seek the Truth\"](http://www.nature.com/polopoly_fs/1.18510!/menu/main/topColumns/topLeftColumn/pdf/526187a.pdf) \n",
    "\n",
    "* Klein & Roodman (2005) [\"Blind Analysis in Nuclear and Particle Physics\"](https://www.pp.rhul.ac.uk/~cowan/stat/annurev.nucl.55.090704.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../graphics/tour_cluster_spec_residuals.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detection\n",
    "\n",
    "* Source detection is different in character from parameter estimation. Initially, we are less interested in the flux of the source than we are in its existence.\n",
    "\n",
    "\n",
    "* Detection is therefore a *model comparison* or *hypothesis testing* problem:\n",
    "\n",
    "  * $H_0$: the \"Null Hypothesis,\" that there is no source present\n",
    "  \n",
    "  * $H_1$: the \"Alternative Hypothesis,\" that there is a source present, with flux $f$ and position $(x,y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Detection\n",
    "\n",
    "* We calculate and compare the Bayesian Evidences for each model, ${\\rm Pr}(d\\,|\\,H_0)$ and ${\\rm Pr}(d\\,|\\,H_1)$\n",
    "\n",
    "\n",
    "* Their ratio gives the relative probability of getting the data under the alternative and null hypotheses (and is equal to the relative probabilities of the hypotheses being true, up to an unknown model prior ratio).\n",
    "\n",
    "\n",
    "* This is an odds ratio (like \"ten to one\") that guides any bet we might want to make (staking, for example, some of our professional reputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prior Dependence\n",
    "\n",
    "* Calculating the Bayesian Evidence ratio involves marginalizing over the alternative hypothesis' model parameters, given the prior PDFs that we assigned when writing down $H_1$\n",
    "\n",
    "\n",
    "* Weakening the prior on the source position and flux (by, for example, expanding their ranges) makes _any given point in parameter space less probable_, the detected model correspondingly more contrived, and the data less likely\n",
    "\n",
    "\n",
    "* So, weaker priors _decrease_ the evidence for $H_1$, making the detection less significant (but only linearly, remember)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Frequentist Detection\n",
    "\n",
    "* Instead of working towards the relative probabilities of two hypotheses, the Frequentist approach is to attempt to reject the null hypothesis by showing that it would be too improbable for the data to have been generated by it\n",
    "\n",
    "\n",
    "* It turns out that the most _powerful_ statistic to use in this hypothesis test is the likelihood ratio\n",
    "\n",
    "\n",
    "> \"Power\" is defined to be the probability that the null hypothesis test is rejected when the alternative hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood Ratios\n",
    "\n",
    "* The procedure is to maximize the likelihood for the parameters of each hypothesis, and form the following test statistic:\n",
    "\n",
    "### $\\;\\;\\;\\;\\;\\;\\;T_d = 2 \\log \\frac{L(\\hat{f},\\,\\hat{x},\\,\\hat{y}\\,;\\,H_1)}{L(H_0)}$\n",
    "\n",
    "\n",
    "* We then inspect the distribution of test statistics $T$ over an ensemble of hypothetical datasets, and compute the $p$-value $P(T_d > T)$\n",
    "\n",
    "\n",
    "* If $p < \\alpha$ we then say that \"we can reject the null hypothesis at the $100\\,\\alpha$ percent confidence level.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Distribution of Test Statistics\n",
    "\n",
    "1) Simulation, in the \"Toy Monte Carlo\" approach: for a range of model parameters, generate large numbers of mock datasets, fit them, and compute $T$ for each one. Pros: reliable. Cons: CPU-intensive, depends on \"parameter ranges\"...\n",
    "\n",
    "\n",
    "2) Approximation: for large dataset size, $T \\sim \\chi^2(\\Delta\\nu)$, where $\\Delta\\nu$ is the difference in the number of degrees of freedom between the two hypotheses (3, in our example). This is [Wilks' Theorem](https://en.wikipedia.org/wiki/Likelihood-ratio_test#Distribution:_Wilks.E2.80.99_theorem)\n",
    "\n",
    "In case 2), we get a huge computational speed-up, but there's a catch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fishing Expeditions\n",
    "\n",
    "The test statistic depends only on the estimated values of the source parameters, $(\\hat{f},\\,\\hat{x},\\,\\hat{y})$, and reports the likelihood ratio between two _discrete_ hypothesis\n",
    "\n",
    "\n",
    "We need to account for the fact that we \"went fishing\" (ie, we _looked elsewhere_) for the source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../graphics/det_xkcd_1.png\">\n",
    "\n",
    "\n",
    "[XKCD \"Significant\"](https://xkcd.com/882), [CC NC-BY 2.5](https://creativecommons.org/licenses/by-nc/2.5/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../graphics/det_xkcd_2.png\" width=40%>\n",
    "\n",
    "\n",
    "[XKCD \"Significant\"](https://xkcd.com/882), [CC NC-BY 2.5](https://creativecommons.org/licenses/by-nc/2.5/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../graphics/det_xkcd_3.png\">\n",
    "\n",
    "\n",
    "[XKCD \"Significant\"](https://xkcd.com/882), [CC NC-BY 2.5](https://creativecommons.org/licenses/by-nc/2.5/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Bonferroni Correction\n",
    "\n",
    "If we carry out $m$ independent \"trials\", and are aiming to detect at the $\\alpha$ confidence level, we would expect to get a positive result by chance in a fraction $\\alpha' = 1 - (1-\\alpha)^m \\lesssim m \\alpha$ of cases\n",
    "\n",
    "\n",
    "Even if the trials are not independent, this last inequality still holds: the \"Bonferroni Correction\" involves comparing $p$-values to a threshold $\\alpha / m$, if you want to test (and report) at the $\\alpha$ confidence level. \n",
    "\n",
    "\n",
    "> This $1/m$ is sometimes referred to as the \"trials factor,\" and the issue described here is known in the statistics literature as the \"multiple comparisons\" problem, and in (astro)particle physics as the \"look elsewhere effect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question:\n",
    "How many discrete hypothesis tests are perfomed:\n",
    "\n",
    "* In the cartoon?\n",
    "\n",
    "* When detecting sources in an astronomical image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Another Question:\n",
    "\n",
    "* Does the simulation approach (1) suffer from the look elsewhere effect as well? \n",
    "\n",
    "* Does the Bayesian approach to detection suffer from the look elsewhere effect?\n",
    "\n",
    "Be prepared to give reasons..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Endnote: Classification and Detection\n",
    "\n",
    "Detection is similar to classification, in that we seek to assign labels to objects (\"detected\" or \"undetected\")\n",
    "\n",
    "\n",
    "Both involve a hypothesis test, and they share some terminology:\n",
    "\n",
    "* _False positives_ arise when the null hypothesis is incorrectly rejected: a \"Type I Error\"\n",
    "\n",
    "* _False negatives_ arise when the null hypothesis is incorrectly retained: a \"Type II Error\"\n",
    "\n",
    "\n",
    "The Bonferroni Correction serves to reduce Type I errors (at the risk of increasing the Type II error rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Post-Hoc Analysis\n",
    "\n",
    "Part of the problem in the cartoon was that more statistical tests were carried out _after the first one was reported._\n",
    "\n",
    "\n",
    "Such \"post-hoc\" (\"after the fact\") \"data dredging\" inflates the number of hypotheses, and care has to be taken.\n",
    "\n",
    "\n",
    "Post-hoc analysis is extremely common in astronomy, since our science is so often driven by new observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Avoiding Type I Errors\n",
    "\n",
    "\n",
    "Models suggested by an intial analysis of the data will naturally fit better. \n",
    "\n",
    "Allowing information from the same dataset to enter twice leads to the primary risk:\n",
    "\n",
    "\n",
    "* Generating false positives (in detection/classification) or \n",
    "* Under-estimating parameter uncertainties (in regression/measurement). \n",
    "\n",
    "\n",
    "The best way to avoid this problem is to _test the new models on new data_\n",
    "\n",
    "> This is the logic behind cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Analyses are Not Immune\n",
    "\n",
    "Recall that the Evidence is maximized by choosing a delta function prior centered on the maximum likelihood parameters\n",
    "\n",
    "\n",
    "The fact that Bayesian analysis necessarily involves writing down the prior PDF _is_ helpful: it should be clear that it's garbage coming out, if everyone can see that it's garbage going in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experimenter Bias\n",
    "\n",
    "A particularly insidious post-hoc analysis problem is \"unconscious experimenter bias:\"\n",
    "\n",
    "\n",
    "* After completing their analysis, a researcher compares their conclusion with those of others\n",
    "\n",
    "* If there is disagreement, the researcher is _more likely_ to try and \"fix the problem\" before publishing\n",
    "\n",
    "* Net result: there is a natural tendency towards \"concordance\" in the literature (see figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../graphics/det_particle-data.png\" width=60%>\n",
    "\n",
    "> Figure from Klein & Roodman (2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Blinding\n",
    "\n",
    "A practical solution to the problem of unconscious experimenter bias is to _blind_ the analysis\n",
    "\n",
    "\n",
    "* The analysis team is prevented from completing their inference and viewing the results until they deem their model complete (or at least, adequate)\n",
    "\n",
    "\n",
    "* The final inference is done once, and the results tagged as \"blinded\"\n",
    "\n",
    "\n",
    "* Post-hoc analysis is then enabled, but with results tagged as \"unblinded\" (since bias could be creeping in again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approaches to Blinding\n",
    "\n",
    "Before \"opening the box\" for the final inference run, one might use a method like:\n",
    "\n",
    "* Hidden signal box: the subset of data believed most likely to contain the signal is removed\n",
    "\n",
    "* Hidden answer: the numerical values of the parameters being measured are hidden (by the analysis and plotting software adding offsets, etc) \n",
    "\n",
    "* Adding/removing data: so that the team don't know whether they have detected a signal or not\n",
    "\n",
    "* Training on a \"pre-scaling\" subset: as in cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Blinding\n",
    "\n",
    "Mitigating against unconscious experimenter bias involves doing some qualitatively different things:\n",
    "\n",
    "\n",
    "* Organizing analyses in teams, and agreeing to abide by rules\n",
    "\n",
    "\n",
    "* Temporarily censoring or adjusting datasets while inferences are developed\n",
    "\n",
    "\n",
    "The results may not be pleasing, but blinding can increase confidence in the robustness of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advertisement\n",
    "\n",
    ". \n",
    "\n",
    "#### KIPAC Workshop, [\"Blind Analysis in High-Stakes Survey Science: When, Why, and How?\"](http://kipac.github.io/Blinding/)\n",
    ".\n",
    "##### March 13-15, 2017 (next Monday-Wednesday)\n",
    ".\n",
    "> Register online, focus on plenaries for high SNR summaries, follow notes remotely"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
