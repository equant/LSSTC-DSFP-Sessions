{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fitting a Straight Line with `SciKit-Learn`\n",
    "\n",
    "Goals:\n",
    "\n",
    "* See the straight line fitting problem solved via the machine learning approach\n",
    "\n",
    "* Understand how model accuracy is quantified, and generalized via cross-validation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further Reading\n",
    "\n",
    "Ivezic Sections 8.1 and 8.2 (linear regression), and Section 8.11 for cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "Straight line fitting is a [linear regression](http://scikit-learn.org/stable/modules/linear_model.html) problem - and an example of predictive learning. \n",
    "\n",
    "A predictive model can be said to have been \"fitted\" to the data when an assumed _loss function_ has been minimized. A popular choice of minimized loss function is the following, corresponding to the method of \"ordinary least squares\":\n",
    "\n",
    "$$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2 $$\n",
    "\n",
    "> While this loss function is derivable from statistical principles, the machine only needs it to be encoded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "If we fit a straight line to a subset of the data (the training set), the accuracy of the linear model's predictions in the remainder of the data (the test set) can be checked. \n",
    "\n",
    "Let's fit some test data with a straight line using the `SciKit-Learn` library, and see how accurately we can make our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Code source: Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Load the boston dataset, and focus on just one attribute: \n",
    "# LSTAT (attribute 12)\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "# Package into design matrix X and target vector y:\n",
    "X = np.atleast_2d(boston.data[:,12]).T\n",
    "y = np.atleast_2d(boston.target).T\n",
    "\n",
    "# Make a training/test split:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "print X_train.shape, X_test.shape\n",
    "print y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_test, y_test, color='black')\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Linear Model\n",
    "\n",
    "`scikit-learn` machine learning models have a common API:\n",
    "\n",
    "* a `fit` method that optimizes the model's internal parameters given training data features and target values\n",
    "\n",
    "* a `predict` method that returns model-predicted target values given test data features\n",
    "\n",
    "* various `score`s for quantifying performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create linear regression model object:\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training set:\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# The coefficients:\n",
    "print(\"Coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scoring\n",
    "\n",
    "* The \"mean squared error\" between the model predictions and the truth is a useful metric: minimizing MSE corresponds to minimizing the \"empirical risk,\" defined as the mean value loss function averaged over the available data samples, where the loss function is quadratic:\n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;{\\rm MSE} = \\mathcal{E} \\left[ (\\hat{y} - y^{\\rm true})^2 \\right] = \\mathcal{E} \\left[ (\\hat{y} - \\bar{y} + \\bar{y} - y^{\\rm true})^2 \\right] = \\mathcal{E} \\left[ (\\hat{y} - \\bar{y})^2 \\right] + (\\bar{y} - y^{\\rm true})^2$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; = {\\rm var}(\\hat{y}) + {\\rm bias}^2(\\hat{y})$\n",
    "\n",
    "\n",
    "* In general, different models reach different balances between the variance and bias of their predictions\n",
    "\n",
    "* A particular choice of loss function leads to a corresponding minimized risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The mean square prediction error:\n",
    "print(\"Training data: MSE = %.2f\"\n",
    "      % np.mean((model.predict(X_train) - y_train) ** 2))\n",
    "print(\"Test data: MSE = %.2f\"\n",
    "      % np.mean((model.predict(X_test) - y_test) ** 2))\n",
    "print \"\"\n",
    "\n",
    "# The \"explained variance\" R2 score: 1 is perfect prediction:\n",
    "print('Training data: R^2 score = %.2f' % model.score(X_train, y_train))\n",
    "print('Test data: R^2 score = %.2f' % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### R2 scores\n",
    "\n",
    "* The \"explained variance\" $R^2$ \"score\" is defined as $(1 - u/v)$, where $u$ is the *regression sum of squares* $\\sum (y_{\\rm true} - y_{\\rm pred})^2$ and $v$ is the *residual sum of squares* $\\sum (y_{\\rm true} - \\overline{y_{\\rm true}})^2)$. \n",
    "\n",
    "\n",
    "* The best possible $R^2$ score is 1.0. A model that has mean squared error equal to the variance in the data gets a score of zero. Models that do systematically worse than this have negative $R^2$ scores.\n",
    "\n",
    "\n",
    "* In general we expect the training score to be higher than the test score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot outputs:\n",
    "plt.scatter(X_test, y_test,  color='black')\n",
    "plt.plot(X_test, model.predict(X_test), color='blue', linewidth=3)\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Questions:\n",
    "\n",
    "* How is this procedure different from previous occasions we have fitted a straight line? \n",
    "\n",
    "* Is it \"Frequentist\" or \"Bayesian\"? Why? \n",
    "\n",
    "* Does it follow the likelihood principle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizing Model Prediction Accuracy\n",
    "\n",
    "* In supervised machine learning the usual goal is to make the most accurate predictions we can - which means neither over-fitting nor under-fitting the data \n",
    "\n",
    "\n",
    "**Question:** In what way are the predictions from an under-fitted model innaccurate? How about an over-fitted model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizing Model Prediction Accuracy\n",
    "\n",
    "* Above, we made one training/test split, and computed the (mean squared) prediction error. The model that minimizes the *generalized prediction error* can be found (approximately) with *cross validation*.\n",
    "\n",
    "\n",
    "* In cross validation we consider multiple training/test splits, and look at the _mean score_ across all of these _\"folds.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../graphics/ml_cross_validation.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "cross_val_score(model, X, y, cv=5, scoring='r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fold Design\n",
    "\n",
    "* How we design the folds matters: we want each subset of the data to be a _fair sample_ of the whole.\n",
    "\n",
    "\n",
    "* In this problem, we want to select the LSTAT values randomly (rather than sequentially), and so we make a `ShuffleSplit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "shuffle_split = ShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
    "cross_val_score(model, X, y, cv=shuffle_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalized Scoring\n",
    "\n",
    "With our 10 fold shuffle splits, we can calculate generalized accuracy scores  - that could be used in a cross-validation model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MSE = cross_val_score(model, X, y, cv=shuffle_split, scoring='neg_mean_squared_error')\n",
    "GE, errGE = -np.mean(MSE), np.std(MSE)/np.sqrt(len(MSE))\n",
    "print \"Generalization error:\", GE, \"+/-\", errGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R2 = cross_val_score(model, X, y, cv=shuffle_split, scoring='r2')\n",
    "meanR2, errR2 = np.mean(R2), np.std(R2)/np.sqrt(len(R2))\n",
    "print \"Generalized R2 score:\", meanR2, \"+/-\", errR2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Expansion\n",
    "\n",
    "* Let's expand our linear model to include some higher order terms (quadratic, cubic etc). This can be done by adding additional feature columns to the design matrix $X$. We are still just predicting $y$, but now we'll be asking for more coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=4)\n",
    "XX = poly.fit_transform(X)\n",
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polymodel = linear_model.LinearRegression()\n",
    "\n",
    "poly_split = ShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
    "\n",
    "R2 = cross_val_score(polymodel, XX, y, cv=poly_split, scoring='r2')\n",
    "\n",
    "poly_meanR2, poly_errR2 = np.mean(R2), np.std(R2)/np.sqrt(len(R2))\n",
    "print \"Polynomial: generalized R^2 score:\", np.round(poly_meanR2, 2), \"+/-\", np.round(poly_errR2, 2)\n",
    "print \"Straight line: generalized R^2 score:\", np.round(meanR2, 2), \"+/-\", np.round(errR2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Checking\n",
    "\n",
    "As usual, it's a good idea to check the model's performance by visualizing its predictions in data space\n",
    "\n",
    "\n",
    "We can make one model prediction for each training set in a set of folds, and plot all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "\n",
    "y_straightline = cross_val_predict(model, X, y, cv=10)\n",
    "y_polynomial = cross_val_predict(polymodel, XX, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot outputs:\n",
    "plt.scatter(X, y,  color='black', alpha=0.1)\n",
    "plt.plot(X, y_straightline, color='blue', linewidth=2, alpha=0.4)\n",
    "plt.plot(X, y_polynomial, color='green', linewidth=2, alpha=0.4)\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In this example, the polynomial degree is a control parameter that needs to be set: we can search this parameter space for the value that gives the highest average cross validation score (or lowest generalization error). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "* The Boston dataset has 13 attributes, more than one of which might contain information about house prices in the city. Let's train a linear model on all these attributes, and see if we can improve our score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a linear model:\n",
    "supermodel = linear_model.LinearRegression()\n",
    "\n",
    "# Use all the data, and set up a 10-fold cross validation run:\n",
    "super_split = ShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
    "\n",
    "# Carry out the cross-validation of the model, training, testing and reporting:\n",
    "R2 = cross_val_score(supermodel, boston.data, boston.target, cv=super_split, scoring='r2')\n",
    "                           \n",
    "# Compute our model prediction accuracy score, for comparison with other models:\n",
    "hpmeanR2, hperrR2 = np.mean(R2), np.std(R2)/np.sqrt(len(R2))\n",
    "print \"Hyperplane: generalized R^2 score:\", np.round(hpmeanR2, 2), \"+/-\", np.round(hperrR2, 2)\n",
    "print \"Straight line: generalized R^2 score:\", np.round(meanR2, 2), \"+/-\", np.round(errR2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What just happened?\n",
    "\n",
    "* We just went from a simple hypothesis (median house price `MEDV` depends on `LSTAT`) to a very much more complex one (house price could depend on all of our 13 measured attributes) in one step. The data analysis is *automated*, in the sense that we simply fed our machine new inputs and it processed them.\n",
    "\n",
    "\n",
    "* Using all our data, we are now better at predicting house prices - *but we have gained no new understanding of how the Boston housing market works.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Endnote\n",
    "\n",
    "* Machine learning algorithms are designed to make good use of big, complex datasets, where there are likely to be many more correlations and connections than we have thought of yet. \n",
    "\n",
    "\n",
    "* In this *data-driven* approach we assume that we will be able to make better predictions by using flexible, \"non-parametric\" methods that scale with the size of the dataset and allow new relationships to emerge empirically."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
