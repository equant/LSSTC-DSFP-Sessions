{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning in Astronomy\n",
    "\n",
    "Goals:\n",
    "\n",
    "* Work through example regression and classification analyses of the SDSS catalog dataset\n",
    "\n",
    "* Understand some more \"score\" metrics and diagnostic visualizations, and carry out model selection by \"cross validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further Reading\n",
    "\n",
    "Ivezic Chapter 8 (regression) and Chapter 9 (classification)\n",
    "\n",
    "> Credit: much of the material in this chunk is based on Josh Bloom's SDSS example notebook, from the 2014 edition of \"Astro Hack Week\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Photometric Redshift Estimation and Quasar Classification\n",
    "\n",
    "Modern wide field surveys are generating very large databases of automatically measured objects, whose error properties may not be well understood. \n",
    "\n",
    "Fast machine learning algorithms are proving to be very useful in such a regime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Photometric Redshift Estimation and Quasar Classification\n",
    "\n",
    "Let's investigate the SDSS photometric object catalog, and look for machine learning solutions to the following two problems:\n",
    "\n",
    "1. Estimating the redshifts of quasars from their photometry (regression)\n",
    "\n",
    "2. Selecting quasars from a background of stars and galaxies (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Aquisition\n",
    "\n",
    "From the SDSS Sky Server we've downloaded two types of photometry (aperature and petrosian), corrected for extinction, for a number of sources with redshifts. Here's the SQL for an example query, that gets us 10000 example quasars:\n",
    "\n",
    "<font color=\"blue\" size=2>\n",
    "<pre>SELECT *,dered_u - mag_u AS diff_u, dered_g - mag_g AS diff_g, dered_r - mag_r AS diff_g, dered_i - mag_i AS diff_i, dered_z - mag_z AS diff_z from\n",
    "(SELECT top 10000\n",
    "objid, ra, dec, dered_u,dered_g,dered_r,dered_i,dered_z,psfmag_u-extinction_u AS mag_u,\n",
    "psfmag_g-extinction_g AS mag_g, psfmag_r-extinction_r AS mag_r, psfmag_i-extinction_i AS mag_i,psfmag_z-extinction_z AS mag_z,z AS spec_z,dered_u - dered_g AS u_g_color, \n",
    "dered_g - dered_r AS g_r_color,dered_r - dered_i AS r_i_color,dered_i - dered_z AS i_z_color,class\n",
    "FROM SpecPhoto \n",
    "WHERE (class = 'QSO') ) as sp\n",
    " </pre>\n",
    "</font>\n",
    "\n",
    "We can do the same for 'STAR's and 'GALAXY's as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "%pylab inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import copy\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Photometric Redshift Estimation\n",
    "\n",
    "* This is a regression problem, to be able to predict the redshift response variable given a number of photometric measurement \"features\"\n",
    "\n",
    "\n",
    "* The SDSS spectroscopic quasar sample provides a _training set_ for the prediction of photometric redshifts\n",
    "\n",
    "\n",
    "* Let's read in our SDSS quasars, and munge them into machine learning inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "qsos = pd.read_csv(\"../examples/SDSScatalog/data/qso10000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"spec_z\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\"])\n",
    "\n",
    "# Clean out extreme colors and bad magnitudes:\n",
    "qsos = qsos[(qsos[\"dered_r\"] > -9999) & (qsos[\"g_r_color\"] > -10) & (qsos[\"g_r_color\"] < 10)]\n",
    "\n",
    "\n",
    "# Response variables: redshift\n",
    "qso_redshifts = qsos[\"spec_z\"]\n",
    "\n",
    "# Features or attributes: photometric measurements\n",
    "qso_features = copy.copy(qsos)\n",
    "del qso_features[\"spec_z\"]\n",
    "qso_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visual Exploration\n",
    "\n",
    "* Over what redshift range do we have quasar spectra?\n",
    "\n",
    "* What structure is there in the _photometric feature space_ that might help us predict redshift?\n",
    "\n",
    "\n",
    "Let's plot all the features, colored by the target redshift, to look for structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "bins =  hist(qso_redshifts.values,bins=100) ; xlabel(\"redshift\") ; ylabel(\"N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Truncate the color at z=2.5 just to keep some contrast.\n",
    "norm = mpl.colors.Normalize(vmin=min(qso_redshifts.values), vmax=2.5)\n",
    "cmap = cm.jet_r\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "# Plot everything against everything else:\n",
    "rez = pd.scatter_matrix(qso_features[0:1000], alpha=0.2, figsize=[15,15], c=m.to_rgba(qso_redshifts.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Data and Target Values\n",
    "\n",
    "Now we have our machine learning inputs (photometric features: colors and magnitudes) and outputs (target redshift values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = qso_features.values  # Data: 9-d feature space\n",
    "y = qso_redshifts.values # Target: redshifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Design matrix shape =\", X.shape)\n",
    "print(\"Response variable vector shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression\n",
    "\n",
    "Let's follow the same procedure as in the [previous chunk (from the `SciKit-Learn` Linear Regression tutorial)](machinelearning2.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Instantiate a \"linear model\"\n",
    "from sklearn import linear_model\n",
    "linear = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the model, using all the attributes:\n",
    "linear.fit(X_train, y_train)\n",
    "\n",
    "# Do the prediction on the test data:\n",
    "y_lr_pred = linear.predict(X_test)\n",
    "\n",
    "# How well did we do? Compute MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_linear = np.sqrt(mean_squared_error(y_test,y_lr_pred))\n",
    "r2_linear = linear.score(X_test, y_test)\n",
    "print(\"Linear regression: MSE = \",mse_linear)\n",
    "print(\"R2 score =\",r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(y_test, y_lr_pred, 'o', alpha=0.2)\n",
    "title(\"Linear Regression - MSE = %.2f\" % mse_linear, fontsize=18)\n",
    "xlabel(\"Spectroscopic Redshift\", fontsize=18)\n",
    "ylabel(\"Photometric Redshift\", fontsize=18)\n",
    "plot([0,7],[0,7], color='red')\n",
    "ylim(0,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Just how bad is this? Here's the MSE from guessing the *average redshift of the training set* for all new objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Naive MSE\", ((1./len(y_train))*(y_train - y_train.mean())**2).sum())\n",
    "print(\"Linear regression: MSE = \",mse_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### *k*-Nearest Neighbor (KNN) Regression\n",
    "\n",
    "Now let's try a non-parametric model:\n",
    "\n",
    "> [\"Regression based on k-nearest neighbors.](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.\"\n",
    "\n",
    "\n",
    "#### Question:\n",
    "\n",
    "What underlying model is implied by the KNN algorithm? How many hidden parameters does it (effectively) have? What choices do we get to make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_scaled = preprocessing.scale(X) # Many methods work better on re-scaled (\"whitened\") X.\n",
    "\n",
    "KNN = neighbors.KNeighborsRegressor(5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "KNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_knn_pred = KNN.predict(X_test)\n",
    "mse_knn = mean_squared_error(y_test,y_knn_pred)\n",
    "r2_knn = KNN.score(X_test, y_test)\n",
    "print(\"MSE (KNN) =\", mse_knn)\n",
    "print(\"R2 score (KNN) =\",r2_knn)\n",
    "print(\"cf.\")\n",
    "print(\"MSE (linear regression) = \",mse_linear)\n",
    "print(\"R2 score (linear regression) =\",r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(y_test, y_knn_pred,'o',alpha=0.2)\n",
    "title(\"k-NN Residuals - MSE = %.2f\" % mse_knn, fontsize=18)\n",
    "xlabel(\"Spectroscopic Redshift\", fontsize=18)\n",
    "ylabel(\"Photometric Redshift\", fontsize=18)\n",
    "plot([0,7], [0,7], color='red')\n",
    "ylim(0,5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tuning the KNN Model\n",
    "\n",
    "* Let's vary the control parameters of the KNN model, to see how good we can make our predictions.\n",
    "\n",
    "* We can see our options in the model `repr`:\n",
    "\n",
    "```\n",
    "KNeighborsRegressor(algorithm='auto',\n",
    "    leaf_size=30, metric='minkowski',      \n",
    "    metric_params=None, n_neighbors=5, \n",
    "    p=2, weights='uniform')\n",
    "```\n",
    "* Let's first make a \"validation curve\" to investigate one parameter: the number of nearest neighbors averaged over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We'll vary the number of neighbors used:\n",
    "param_name = \"n_neighbors\"\n",
    "param_range = np.array([1,2,4,8,16,32,64])\n",
    "\n",
    "# Compute our cv scores for the above range of the no. of neighbors:\n",
    "from sklearn.model_selection import validation_curve\n",
    "training_scores, validation_scores = validation_curve(KNN, X_scaled, y,\n",
    "                                                      param_name=param_name,\n",
    "                                                      param_range=param_range, \n",
    "                                                      cv=3, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_validation_curve(param_name,parameter_values, training_scores, validation_scores):\n",
    "    training_scores_mean = np.mean(training_scores, axis=1)\n",
    "    training_scores_std = np.std(training_scores, axis=1)\n",
    "    validation_scores_mean = np.mean(validation_scores, axis=1)\n",
    "    validation_scores_std = np.std(validation_scores, axis=1)\n",
    "\n",
    "    plt.fill_between(parameter_values, training_scores_mean - training_scores_std,\n",
    "                     training_scores_mean + training_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(parameter_values, validation_scores_mean - validation_scores_std,\n",
    "                     validation_scores_mean + validation_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(parameter_values, training_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training R2 score\")\n",
    "    plt.plot(parameter_values, validation_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation R2 score\")\n",
    "    plt.ylim(validation_scores_mean.min() - .1, training_scores_mean.max() + .1)\n",
    "    plt.xlabel(param_name, fontsize=18)\n",
    "    plt.legend(loc=\"best\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_validation_curve(param_name, param_range, training_scores, validation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Question:\n",
    "\n",
    "Can you explain the shapes of these two curves? Talk to your neighbor for a few minutes, and be prepared to suggest reasons for a) the rise and fall of the cross validation score and b) the monotonic decrease in training score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model tuning with `GridSearchCV`\n",
    "\n",
    "* Now, let's see if we can do better by varying some other KNN options as well - in a *grid search*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': np.array([1,2,4,8,16,32,64]),\n",
    "                  'weights': ['uniform','distance'],\n",
    "                       'p' : np.array([1,2])}\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "KNN_tuned = GridSearchCV(KNN, param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A `GridSearchCV` object behaves just like a model, except it carries out a cross-validation while fitting:\n",
    "\n",
    "<img src=\"../graphics/ml_grid_search_cross_validation.svg\" width=70% align='center'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "KNN_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_knn_tuned_pred = KNN_tuned.predict(X_test)\n",
    "\n",
    "mse_knn_tuned = mean_squared_error(y_test,y_knn_tuned_pred)\n",
    "r2_knn_tuned = KNN_tuned.score(X_test, y_test)\n",
    "\n",
    "print(\"MSE (tuned KNN) =\", mse_knn_tuned)\n",
    "print(\"R2 score (tuned KNN) =\", r2_knn_tuned)\n",
    "print(\"cf.\")\n",
    "print(\"MSE (KNN) = \", mse_knn)\n",
    "print(\"R2 score (KNN) =\", r2_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which are the best KNN control parameters we found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNN_tuned.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This value of `n_neighbors` is consistent with the peak in cross-validation score in the validation curve plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalization Error\n",
    "\n",
    "Notice that all the above tuning happened while training on a single split (`X_train` and `y_train`).\n",
    "\n",
    "\n",
    "It's possible that that one particular fold prefers a slightly different set of model parameters than a different one - so to assess our generalization error, we need a further level of cross-validation.\n",
    "\n",
    "\n",
    "We can do this by passing a `GridSearchCV` model to the cross validation score calculator. This will take a few minutes, as the grid search is carried out for each CV fold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "R2 = cross_val_score(KNN_tuned, X_scaled, y, cv=3, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "meanR2, errR2 = np.mean(R2), np.std(R2)/np.sqrt(len(R2))\n",
    "print('Mean score:', meanR2, '+/-', errR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notes\n",
    "\n",
    "* Optimizing over control parameters (or hyper parameters) with grid search cross validation is a form of model selection.\n",
    "\n",
    "\n",
    "* When presented with new data samples (photometry), and asked to predict the target response variables (photometric redshifts), we'll need a trained machine that has not been *over-fitted* to the training data.\n",
    "\n",
    "\n",
    "* Minimizing and estimating the generalization error is a way to reduce the risk of getting this prediction wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's finish off our photo-z machine learning tool, by:\n",
    "\n",
    "\n",
    "* Choosing the best model (from our cross-validation analysis)\n",
    "\n",
    "* Training it on the whole of the training dataset\n",
    "\n",
    "* Using it to predict the redshifts of the objects in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNNz = KNN_tuned.best_estimator_\n",
    "\n",
    "KNNz.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = 571\n",
    "one_new_quasar = X_test[j,:].reshape(1, -1)\n",
    "zphoto = KNNz.predict(one_new_quasar)\n",
    "zspec = y_test[j]\n",
    "print(\"True redshift cf. KNN photo-z:\", zspec, ' cf.', zphoto[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "zspec = y_test\n",
    "zphoto = KNNz.predict(X_test)\n",
    "\n",
    "plot(zspec, zphoto,'o',alpha=0.1)\n",
    "title(\"KNNz performance\", fontsize=18)\n",
    "xlabel(\"Spectroscopic Redshift\", fontsize=18)\n",
    "ylabel(\"Photometric redshift\", fontsize=18)\n",
    "plot([0,7], [0,7], color='red')\n",
    "ylim(0,5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quasar Classification with Random Forest\n",
    "\n",
    "* Let's switch gears and do a 3-class classification problem: star, galaxy, or QSO.\n",
    "\n",
    "\n",
    "* A very good general-purpose classification (and regression!) algorithm is \"Random Forest.\" See [this yhat blog post](http://blog.yhathq.com/posts/random-forests-in-python.html) for a nice high level introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "\n",
    "* From the `Scikit-Learn` docs: [\"A random forest is a meta estimator that fits a number of *decision tree classifiers* on various sub-samples of the dataset, and uses averaging to improve the predictive accuracy and control over-fitting.\"](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "\n",
    "* Decision trees encode sequences of \"cuts\" in the data, leading to the separation of the samples into groups to which labels are assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Trees\n",
    "\n",
    "<img src=\"../graphics/ml_yhat_decision_tree_example.png\" width=60%>\n",
    "\n",
    "> [yhat.com \"Random Forests in Python\"](http://blog.yhathq.com/posts/random-forests-in-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forests of Random Trees\n",
    "\n",
    "<img src=\"../graphics/ml_yhat_a_random_forest.png\" width=60%>\n",
    "\n",
    "> [yhat.com \"Random Forests in Python\"](http://blog.yhathq.com/posts/random-forests-in-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forests of Random Trees\n",
    "\n",
    "\n",
    "* Random decision trees are random in that they:\n",
    "\n",
    "  * Are set up with random cut thresholds, and random feature-ordering\n",
    "  \n",
    "  * Are given random bootstrap sub-samples of the data\n",
    "  \n",
    "  \n",
    "* Most trees in a random forest are _terrible_ classifiers: but we only need a few with high weight to dominate the average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quasar Classification with Random Forest\n",
    "\n",
    "* Let's read in equal numbers of all three types of data, clean them up, and set $y$ equal to the classification label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "all_sources = pd.read_csv(\"../examples/SDSSCatalog/data/qso10000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"])[:1000]\n",
    "\n",
    "all_sources = all_sources.append(pd.read_csv(\"../examples/SDSSCatalog/data/star1000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"]))\n",
    "\n",
    "all_sources = all_sources.append(pd.read_csv(\"../examples/SDSSCatalog/data/galaxy1000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"]))\n",
    "\n",
    "all_sources = all_sources[(all_sources[\"dered_r\"] > -9999) & (all_sources[\"g_r_color\"] > -10) & (all_sources[\"g_r_color\"] < 10)]\n",
    "\n",
    "all_labels = all_sources[\"class\"]\n",
    "\n",
    "all_features = copy.copy(all_sources)\n",
    "del all_features[\"class\"]\n",
    "\n",
    "X = copy.copy(all_features.values)\n",
    "y = copy.copy(all_labels.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Data and Target Labels\n",
    "\n",
    "In classification problems the target values are _categorical_ \"labels\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Feature vector shape =\", X.shape)\n",
    "print(\"Class label vector shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(y, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visual Exploration\n",
    "\n",
    "What structure can we see in the data? Let's plot all the features as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yy = all_labels.values.copy()\n",
    "yy[yy==\"QSO\"] = 0.0    # Red\n",
    "yy[yy==\"STAR\"] = 0.5   # Green\n",
    "yy[yy==\"GALAXY\"] = 1.0 # Blue\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=min(yy), vmax=max(yy))\n",
    "cmap = cm.jet_r\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "rez = pd.scatter_matrix(all_features, alpha=0.2, figsize=[15,15], c=m.to_rgba(yy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forest feature importances\n",
    "\n",
    "* Each decision tree in the Random Forest focuses on a different combination of features\n",
    "\n",
    "* One output of the fitted model is an indication of which features are most important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(zip(all_sources.columns.values,rf.feature_importances_),key=lambda q: q[1],reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification Accuracy\n",
    "\n",
    "* The accuracy of a classifier is the _fraction of predictions made that are correct_. \n",
    "\n",
    "* Ensemble classifiers like Random Forest provide an `oob_score`, the mean \"Out of Bag\" prediction accuracy\n",
    "\n",
    "* Each decision tree in the ensemble is only working on a subset of the data, so it can track its accuracy with the data not in its own bag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Report the \"out of bag\" accuracy score:\n",
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This score _looks_ good, but this is the accuracy on the _training set_.\n",
    "\n",
    "\n",
    "#### Question:\n",
    "How should we estimate the generalized accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classifier Tuning with GridSearchCV\n",
    "\n",
    "As before, this will take a few minutes, as the model selection is carried out on each fold within the training set... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameter values to try:\n",
    "parameters = {'n_estimators':(50,100,200), \"max_features\": [\"auto\",3],\n",
    "              'criterion':[\"gini\",\"entropy\"], \"min_samples_leaf\": [1,2]}\n",
    "\n",
    "# Training/test split:\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Do a grid search to find the highest 3-fold cross-validation score:\n",
    "rf_tuned = GridSearchCV(rf, parameters, cv=3, verbose=1)\n",
    "RFselector = rf_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Print the best score and estimator:\n",
    "print('Best OOB score:', RFselector.best_score_)\n",
    "print(RFselector.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question:\n",
    "\n",
    "Would you be satisfied with a 95% successful classification fraction? Can you suggest some more useful scores to optimize? (Hint: imagine using a classifier to select a sample of *follow-up targets*.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confusion Matrices\n",
    "\n",
    "One way of visualizing classification accuracy across a range of labels is via a *confusion matrix*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make predictions given the test data:\n",
    "y_pred = RFselector.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix', fontsize=18)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label', fontsize=18)\n",
    "plt.xlabel('Predicted label', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ROC Curves\n",
    "\n",
    "Each Random Forest output label comes with a *classification probability*, computed from the results of the whole forest. \n",
    "\n",
    "To select a sample of classified objects, one can choose a selection threshold in this class probability, and only keep objects with higher probability than this threshold.\n",
    "\n",
    "\n",
    "The availability of a class probability leads to an important diagnostic: the \"Receiver Operating Characteristic\" or [\"ROC\" curve](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC Curves\n",
    "\n",
    "These show a classifier's *true positive rate* (TPR) plotted against the *false positive rate* (FPR), as the selection threshold is varied.\n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;{\\rm TPR} = \\frac{TP}{TP + FN}$  the fraction of actual 1's that are classified as 1's.\n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;{\\rm FPR} = \\frac{FP}{FP + TN}$  the fraction of actual 0's that are classified as 1's.  \n",
    "\n",
    "> TPR = \"Recall\", \"Completeness\". FPR = \"Fall-out\".\n",
    "> In astronomy, we are also (very) interested in \"Precision,\" $\\frac{TP}{FP + TP}$, which is related to \"Purity\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC Curves\n",
    "\n",
    "Typically, classifiers have control parameters that affect both the TPR and FPR (often improving one at the expense of the other), so the ROC curve is a good tool for investigating these parameters. \n",
    "\n",
    "\n",
    "Likewise, ROC curves provide a very good way to compare different classifiers, via the \"area under the curve\" (and above the random classifier TPR=FPR line). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RFselector ROC Curves\n",
    "\n",
    "Let's use the `SciKit-Learn` utilities to plot an ROC curve for our RFselector, and quantify its performance via the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classify the test data, and store the classification probabilities:\n",
    "BestRFselector = RFselector.best_estimator_\n",
    "y_prob = BestRFselector.fit(X_train, y_train).predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and area under curve (AUC) for each class:\n",
    "labels = BestRFselector.classes_ # ['GALAXY', 'QSO', 'STAR'] - the order is decided by the machine!\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i,label in enumerate(labels):\n",
    "    fpr[label], tpr[label], _ = roc_curve(y_test, y_prob[:, i], pos_label=label)\n",
    "    roc_auc[label] = auc(fpr[label], tpr[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lw = 2\n",
    "colors = {'QSO':'red', 'STAR':'green', 'GALAXY':'blue'}\n",
    "for label in labels:\n",
    "    plot(fpr[label], tpr[label], color=colors[label],\n",
    "         lw=lw, label='%s (AUC = %0.3f)' % (label, roc_auc[label]))\n",
    "plot([0, 1], [0, 1], color='gray', lw=lw, linestyle='--')\n",
    "xlim([0.0, 1.0])\n",
    "ylim([0.0, 1.05])\n",
    "xlabel('False Positive Rate', fontsize=18)\n",
    "ylabel('True Positive Rate', fontsize=18)\n",
    "title('RFselector ROC Curve', fontsize=18)\n",
    "legend(loc=\"lower right\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Endnotes\n",
    "\n",
    "\n",
    "The `scikit-learn` package makes it easy to experiment with various machine learning algorithms, and make non-parametric models that (via cross-validation) have high generalized prediction accuracy.\n",
    "\n",
    "(These models may not provide parameter uncertainties or even much new understanding of the dataset, but this may not be your priority.)\n",
    "\n",
    "\n",
    "Other frameworks exist: Google's [\"TensorFlow\"](https://www.tensorflow.org/tutorials/deep_cnn) is worth investigating, both for general ML, and also for its _neural network_ support. PGMs, as well as the ML basics shown here, will reappear"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
